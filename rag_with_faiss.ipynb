{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "cannot assign to expression here. Maybe you meant '==' instead of '='? (3407819970.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[2], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    1+ 1 =daf\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m cannot assign to expression here. Maybe you meant '==' instead of '='?\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "import json\n",
    "import faiss\n",
    "import numpy as np\n",
    "from openai import OpenAI\n",
    "import traceback\n",
    "import openai  # 추가\n",
    "\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# .env 파일 로드\n",
    "load_dotenv()\n",
    "\n",
    "# API_KEY 값을 가져옴\n",
    "openai_api_key = os.getenv(OPENAI_API_KEY)\n",
    "upstage_api_key = os.getenv(UPSATGE_API_KEY)\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = openai_api_key\n",
    "\n",
    "# OpenAI 클라이언트 (Upstage API)\n",
    "client = OpenAI(\n",
    "    api_key= upstage_api_key,\n",
    "    base_url=\"https://api.upstage.ai/v1/solar\"\n",
    ")\n",
    "\n",
    "# FAISS 인덱스 로드\n",
    "index = faiss.read_index(\"knn_index_cosine.faiss\")\n",
    "\n",
    "# chunk-doc 매핑 파일 로드\n",
    "with open(\"chunk_mappings.json\", \"r\") as f:\n",
    "    chunk_doc_mapping = json.load(f)\n",
    "    \n",
    "\n",
    "\n",
    "# Upstage API를 이용한 임베딩 생성\n",
    "def get_embedding(query_str):\n",
    "    query_result = client.embeddings.create(\n",
    "        model=\"solar-embedding-1-large-passage\",\n",
    "        input=query_str\n",
    "    ).data[0].embedding\n",
    "    return np.array(query_result).reshape(1, -1)\n",
    "\n",
    "def normalize(embeddings):\n",
    "    norms = np.linalg.norm(embeddings, axis=1, keepdims=True)\n",
    "    return embeddings / norms\n",
    "\n",
    "def normalize(embedding):\n",
    "    return embedding / np.linalg.norm(embedding)\n",
    "\n",
    "# FAISS에서 벡터 유사도를 이용한 검색\n",
    "def dense_retrieve(query_str, size=5):\n",
    "    # 쿼리 임베딩 생성\n",
    "    query_embedding = get_embedding(query_str)\n",
    "\n",
    "    # FAISS에서 유사한 k개의 문서 검색\n",
    "    distances, indices = index.search(query_embedding, size)\n",
    "\n",
    "    # 검색된 chunk들 가져오기\n",
    "    retrieved_context = []\n",
    "    for idx in indices[0]:\n",
    "        chunk_info = chunk_doc_mapping[idx]\n",
    "        retrieved_context.append(chunk_info['content'])\n",
    "    \n",
    "    return retrieved_context\n",
    "\n",
    "# OpenAI API 사용 준비 (이미 설정된 api_key와 base_url을 사용)\n",
    "llm_model = \"gpt-3.5-turbo-1106\"\n",
    "\n",
    "# RAG 구현을 위한 LLM 프롬프트 설정\n",
    "persona_qa = \"\"\"\n",
    "## Role: 과학 상식 전문가\n",
    "## Instructions\n",
    "- 사용자의 이전 메시지 정보 및 주어진 Reference 정보를 활용하여 간결하게 답변을 생성한다.\n",
    "- 주어진 검색 결과 정보로 대답할 수 없는 경우는 정보가 부족해서 답을 할 수 없다고 대답한다.\n",
    "- 한국어로 답변을 생성한다.\n",
    "\"\"\"\n",
    "\n",
    "# LLM과 FAISS 기반 검색을 활용한 RAG 구현\n",
    "def answer_question(messages):\n",
    "    response = {\"standalone_query\": \"\", \"topk\": [], \"references\": [], \"answer\": \"\"}\n",
    "    \n",
    "    if not messages:\n",
    "        print(\"Error: No input messages provided.\")\n",
    "        return response\n",
    "\n",
    "    msg = [{\"role\": \"system\", \"content\": persona_qa}] + messages\n",
    "    try:\n",
    "        result = client.chat.completions.create(\n",
    "            model=llm_model,\n",
    "            messages=msg,\n",
    "            temperature=0,\n",
    "            seed=1,\n",
    "            timeout=10\n",
    "        )\n",
    "    except Exception as e:  # 모든 예외를 처리\n",
    "        print(f\"Error: {e}\")\n",
    "        return response\n",
    "\n",
    "\n",
    "    # 검색이 필요한 경우 검색 호출 후 결과를 활용\n",
    "    if \"standalone_query\" in result.choices[0].message.tool_calls[0].function.arguments:\n",
    "        query_str = result.choices[0].message.tool_calls[0].function.arguments['standalone_query']\n",
    "        \n",
    "        # FAISS를 사용하여 문서 검색\n",
    "        search_result = dense_retrieve(query_str, 5)\n",
    "\n",
    "        # 검색 결과를 LLM 답변에 반영\n",
    "        response[\"standalone_query\"] = query_str\n",
    "        response[\"references\"] = search_result\n",
    "        content = json.dumps(search_result)\n",
    "        messages.append({\"role\": \"assistant\", \"content\": content})\n",
    "\n",
    "        # 검색된 내용을 기반으로 LLM에 답변 생성 요청\n",
    "        try:\n",
    "            qaresult = client.chat.completions.create(\n",
    "                model=llm_model,\n",
    "                messages=messages,\n",
    "                temperature=0,\n",
    "                seed=1,\n",
    "                timeout=30\n",
    "            )\n",
    "        except Exception as e:\n",
    "            traceback.print_exc()\n",
    "            return response\n",
    "\n",
    "        response[\"answer\"] = qaresult.choices[0].message.content\n",
    "    return response\n",
    "\n",
    "# 테스트 데이터로 RAG 실행\n",
    "test_query = \"금성이 다른 행성들보다 밝게 보이는 이유는 무엇인가요?\"\n",
    "messages = [{\"role\": \"user\", \"content\": test_query}]\n",
    "response = answer_question(messages)\n",
    "\n",
    "# 결과 출력\n",
    "print(f\"Query: {response['standalone_query']}\")\n",
    "print(f\"Answer: {response['answer']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "cannot assign to expression here. Maybe you meant '==' instead of '='? (191897223.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[3], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    1+ 1 =daf\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m cannot assign to expression here. Maybe you meant '==' instead of '='?\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "import json\n",
    "import faiss\n",
    "import numpy as np\n",
    "from openai import OpenAI\n",
    "import traceback\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# .env 파일 로드\n",
    "load_dotenv()\n",
    "\n",
    "# API_KEY 값을 가져옴\n",
    "openai_api_key = os.getenv(OPENAI_API_KEY)\n",
    "upstage_api_key = os.getenv(UPSATGE_API_KEY)\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = openai_api_key\n",
    "\n",
    "# Upstage API 클라이언트 설정\n",
    "client = OpenAI(\n",
    "    api_key= upstage_api_key,\n",
    "    base_url=\"https://api.upstage.ai/v1/solar\"\n",
    ")\n",
    "\n",
    "# FAISS 인덱스 로드\n",
    "index = faiss.read_index(\"knn_index_cosine.faiss\")\n",
    "\n",
    "# 문서와 Chunk ID 매핑 파일 로드\n",
    "with open(\"chunk_mappings.json\", \"r\") as f:\n",
    "    chunk_doc_mapping = json.load(f)\n",
    "\n",
    "# 코사인 거리 적용을 위해 벡터 정규화 함수 정의\n",
    "def normalize(embedding):\n",
    "    return embedding / np.linalg.norm(embedding)\n",
    "\n",
    "# Upstage API를 이용하여 임베딩 생성\n",
    "def get_embedding(query_str):\n",
    "    query_result = client.embeddings.create(\n",
    "        model=\"solar-embedding-1-large-passage\",\n",
    "        input=query_str\n",
    "    ).data[0].embedding\n",
    "    query_embedding = np.array(query_result).reshape(1, -1)\n",
    "    \n",
    "    # 임베딩을 L2 정규화하여 코사인 거리 적용\n",
    "    return normalize(query_embedding)\n",
    "\n",
    "# FAISS를 통한 검색 (KNN 검색)\n",
    "def dense_retrieve(query_str, size=5):\n",
    "    # 쿼리 임베딩 생성\n",
    "    query_embedding = get_embedding(query_str)\n",
    "    \n",
    "    # FAISS에서 유사한 k개의 문서 검색 (코사인 거리를 사용)\n",
    "    distances, indices = index.search(query_embedding, size)\n",
    "\n",
    "    # 검색된 chunk 정보 가져오기\n",
    "    retrieved_context = []\n",
    "    for idx in indices[0]:\n",
    "        chunk_info = chunk_doc_mapping[idx]\n",
    "        retrieved_context.append(chunk_info['content'])\n",
    "    \n",
    "    return retrieved_context\n",
    "\n",
    "# OpenAI API 사용 설정\n",
    "os.environ[\"OPENAI_API_KEY\"] = openai_api_key\n",
    "llm_model = \"gpt-3.5-turbo-1106\"\n",
    "client = OpenAI()\n",
    "\n",
    "# RAG 구현에 필요한 LLM 프롬프트 설정\n",
    "persona_qa = \"\"\"\n",
    "## Role: 과학 상식 전문가\n",
    "## Instructions\n",
    "- 사용자의 이전 메시지 정보 및 주어진 Reference 정보를 활용하여 간결하게 답변을 생성한다.\n",
    "- 주어진 검색 결과 정보로 대답할 수 없는 경우는 정보가 부족해서 답을 할 수 없다고 대답한다.\n",
    "- 한국어로 답변을 생성한다.\n",
    "\"\"\"\n",
    "\n",
    "# LLM과 FAISS 기반 검색을 활용한 RAG 구현\n",
    "def answer_question(messages):\n",
    "    response = {\"standalone_query\": \"\", \"topk\": [], \"references\": [], \"answer\": \"\"}\n",
    "\n",
    "    # 질의 분석 및 LLM 활용\n",
    "    msg = [{\"role\": \"system\", \"content\": persona_qa}] + messages\n",
    "    try:\n",
    "        result = client.chat.completions.create(\n",
    "            model=llm_model,\n",
    "            messages=msg,\n",
    "            temperature=0,\n",
    "            seed=1,\n",
    "            timeout=10\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return response\n",
    "\n",
    "    # 검색이 필요한 경우 FAISS 검색 수행\n",
    "    if result.choices[0].message.tool_calls:\n",
    "        tool_call = result.choices[0].message.tool_calls[0]\n",
    "        function_args = json.loads(tool_call.function.arguments)\n",
    "        standalone_query = function_args.get(\"standalone_query\")\n",
    "\n",
    "        # FAISS 검색으로 문서 검색\n",
    "        search_result = dense_retrieve(standalone_query, 3)\n",
    "\n",
    "        response[\"standalone_query\"] = standalone_query\n",
    "        retrieved_context = []\n",
    "        for i, rst in enumerate(search_result):\n",
    "            retrieved_context.append(rst)\n",
    "            response[\"topk\"].append(i)\n",
    "            response[\"references\"].append({\"content\": rst})\n",
    "\n",
    "        content = json.dumps(retrieved_context)\n",
    "        messages.append({\"role\": \"assistant\", \"content\": content})\n",
    "        \n",
    "        # 검색된 내용을 기반으로 LLM에 답변 생성 요청\n",
    "        try:\n",
    "            qaresult = client.chat.completions.create(\n",
    "                model=llm_model,\n",
    "                messages=messages,\n",
    "                temperature=0,\n",
    "                seed=1,\n",
    "                timeout=30\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}\")\n",
    "            return response\n",
    "        response[\"answer\"] = qaresult.choices[0].message.content\n",
    "\n",
    "    # 검색이 필요하지 않은 경우 바로 답변 생성\n",
    "    else:\n",
    "        response[\"answer\"] = result.choices[0].message.content\n",
    "\n",
    "    return response\n",
    "\n",
    "# 테스트 데이터로 RAG 실행\n",
    "test_query = \"금성이 다른 행성들보다 밝게 보이는 이유는 무엇인가요?\"\n",
    "messages = [{\"role\": \"user\", \"content\": test_query}]\n",
    "response = answer_question(messages)\n",
    "\n",
    "# 결과 출력\n",
    "print(f\"Query: {response['standalone_query']}\")\n",
    "print(f\"Answer: {response['answer']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "cannot assign to expression here. Maybe you meant '==' instead of '='? (3887020913.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[4], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    1+ 1 =daf\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m cannot assign to expression here. Maybe you meant '==' instead of '='?\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import faiss\n",
    "from openai import OpenAI  # Upstage API 사용\n",
    "\n",
    "def normalize(embeddings):\n",
    "    norms = np.linalg.norm(embeddings, axis=1, keepdims=True)\n",
    "    return (embeddings / norms).astype(np.float32)  # float32로 변환\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# .env 파일 로드\n",
    "load_dotenv()\n",
    "\n",
    "# API_KEY 값을 가져옴\n",
    "openai_api_key = os.getenv(OPENAI_API_KEY)\n",
    "upstage_api_key = os.getenv(OPENAI_API_KEY)\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = openai_api_key\n",
    "\n",
    "# Upstage API 클라이언트 설정\n",
    "client = OpenAI(\n",
    "    api_key= upstage_api_key,\n",
    "    base_url=\"https://api.upstage.ai/v1/solar\"\n",
    ")\n",
    "\n",
    "index = faiss.read_index(\"knn_index_cosine.faiss\")\n",
    "\n",
    "# gpu_index = faiss.index_cpu_to_gpu(res, 0, index)\n",
    "\n",
    "with open(\"chunk_mappings.json\", \"r\") as f:\n",
    "    chunk_doc_mapping = json.load(f)\n",
    "\n",
    "# 쿼리 입력 후 Upstage API를 통해 임베딩 생성\n",
    "query = \"나무의 분류에 대해 조사해 보기 위한 방법은?\"\n",
    "query_result = client.embeddings.create(\n",
    "    model=\"solar-embedding-1-large-passage\",\n",
    "    input=query\n",
    ").data[0].embedding\n",
    "\n",
    "query_embedding = np.array(query_result).reshape(1, -1)\n",
    "\n",
    "# 쿼리 임베딩 정규화\n",
    "normalized_query = normalize(query_embedding)\n",
    "\n",
    "k = 5\n",
    "distances, indices = index.search(normalized_query, k)\n",
    "\n",
    "retrieved_chunks = []\n",
    "for idx in indices[0]:\n",
    "    chunk_info = chunk_doc_mapping[idx]\n",
    "    content = chunk_info['content']\n",
    "    doc_id = chunk_info['doc_id']\n",
    "    retrieved_chunks.append((query, content))\n",
    "    print(f\"검색된 Chunk ID: {chunk_info['chunk_id']}\")\n",
    "    print(f\"연관된 문서 ID: {doc_id}\")\n",
    "    print(f\"Chunk 내용: {content}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "os.environ[\"OPENAI_API_KEY\"] = openai_api_key\n",
    "llm_model = \"gpt-3.5-turbo-1106\"\n",
    "client = OpenAI()\n",
    "\n",
    "def answer_question(messages):\n",
    "    # 함수 출력 초기화\n",
    "    response = {\"standalone_query\": \"\", \"topk\": [], \"references\": [], \"answer\": \"\"}\n",
    "    \n",
    "    # 질의 분석 및 검색 이외의 질의 대응을 위한 LLM 활용\n",
    "    msg = [{\"role\": \"system\", \"content\": persona_function_calling}] + messages\n",
    "    try:\n",
    "        result = client.chat.completions.create(\n",
    "            model=llm_model,\n",
    "            messages=msg,\n",
    "            temperature=0,\n",
    "            seed=1,\n",
    "            timeout=10\n",
    "        )\n",
    "    except Exception as e:\n",
    "        traceback.print_exc()\n",
    "        return response\n",
    "\n",
    "    # 검색이 필요한 경우 검색 호출후 결과를 활용하여 답변 생성\n",
    "    if result.choices[0].message.tool_calls:\n",
    "        tool_call = result.choices[0].message.tool_calls[0]\n",
    "        function_args = json.loads(tool_call.function.arguments)\n",
    "        standalone_query = function_args.get(\"standalone_query\")\n",
    "\n",
    "        # 검색 결과 추출\n",
    "        search_result = get_retrieved_chunks(standalone_query)\n",
    "\n",
    "        response[\"standalone_query\"] = standalone_query\n",
    "        retrieved_context = []\n",
    "        for i, rst in enumerate(search_result):\n",
    "            retrieved_context.append(rst[1])\n",
    "            response[\"topk\"].append(i)\n",
    "            response[\"references\"].append({\"content\": rst[1]})\n",
    "\n",
    "        content = json.dumps(retrieved_context)\n",
    "        messages.append({\"role\": \"assistant\", \"content\": content})\n",
    "        msg = [{\"role\": \"system\", \"content\": persona_qa}] + messages\n",
    "        try:\n",
    "            qaresult = client.chat.completions.create(\n",
    "                model=llm_model,\n",
    "                messages=msg,\n",
    "                temperature=0,\n",
    "                seed=1,\n",
    "                timeout=30\n",
    "            )\n",
    "        except Exception as e:\n",
    "            traceback.print_exc()\n",
    "            return response\n",
    "        response[\"answer\"] = qaresult.choices[0].message.content\n",
    "\n",
    "    else:\n",
    "        response[\"answer\"] = result.choices[0].message.content\n",
    "\n",
    "    return response\n",
    "\n",
    "# 평가를 위한 파일을 읽어서 각 평가 데이터에 대해서 결과 추출후 파일에 저장\n",
    "def eval_rag(eval_filename, output_filename):\n",
    "    with open(eval_filename) as f, open(output_filename, \"w\") as of:\n",
    "        idx = 0\n",
    "        for line in f:\n",
    "            if idx >= 3:  # 3번까지만 반복\n",
    "                break\n",
    "            j = json.loads(line)\n",
    "            print(f'Test {idx}\\nQuestion: {j[\"msg\"]}')\n",
    "            response = answer_question(j[\"msg\"])\n",
    "            print(f'Answer: {response[\"answer\"]}\\n')\n",
    "\n",
    "            # 대회 score 계산은 topk 정보를 사용, answer 정보는 LLM을 통한 자동평가시 활용\n",
    "            output = {\"eval_id\": j[\"eval_id\"], \"standalone_query\": response[\"standalone_query\"], \"topk\": response[\"topk\"], \"answer\": response[\"answer\"], \"references\": response[\"references\"]}\n",
    "            of.write(f'{json.dumps(output, ensure_ascii=False)}\\n')\n",
    "            idx += 1\n",
    "            \n",
    "            \n",
    "            \n",
    "    # 평가 데이터에 대해서 결과 생성 - 파일 포맷은 jsonl이지만 파일명은 csv 사용\n",
    "eval_rag(\"/ir/data/eval.jsonl\", \"sample_submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "cannot assign to expression here. Maybe you meant '==' instead of '='? (346362905.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[5], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    1+ 1 =daf\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m cannot assign to expression here. Maybe you meant '==' instead of '='?\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import json\n",
    "import numpy as np\n",
    "import faiss\n",
    "from openai import OpenAI  # Upstage API 사용\n",
    "\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# .env 파일 로드\n",
    "load_dotenv()\n",
    "\n",
    "# API_KEY 값을 가져옴\n",
    "openai_api_key = os.getenv(OPENAI_API_KEY)\n",
    "upstage_api_key = os.getenv(OPENAI_API_KEY)\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = openai_api_key\n",
    "\n",
    "def normalize(embeddings):\n",
    "    norms = np.linalg.norm(embeddings, axis=1, keepdims=True)\n",
    "    return (embeddings / norms).astype(np.float32)  # float32로 변환\n",
    "\n",
    "# Upstage API 클라이언트 설정\n",
    "client = OpenAI(\n",
    "    api_key= upstage_api_key,\n",
    "    base_url=\"https://api.upstage.ai/v1/solar\"\n",
    ")\n",
    "\n",
    "index = faiss.read_index(\"knn_index_cosine.faiss\")\n",
    "\n",
    "# 문서 ID 및 chunk ID 매핑 파일 로드\n",
    "with open(\"chunk_mappings.json\", \"r\") as f:\n",
    "    chunk_doc_mapping = json.load(f)\n",
    "\n",
    "# 쿼리 입력 후 Upstage API를 통해 임베딩 생성\n",
    "def get_retrieved_chunks(query, k=5):\n",
    "    query_result = client.embeddings.create(\n",
    "        model=\"solar-embedding-1-large-passage\",\n",
    "        input=query\n",
    "    ).data[0].embedding\n",
    "\n",
    "    query_embedding = np.array(query_result).reshape(1, -1)\n",
    "\n",
    "    # 쿼리 임베딩 정규화\n",
    "    normalized_query = normalize(query_embedding)\n",
    "\n",
    "    # FAISS 검색 실행\n",
    "    distances, indices = index.search(normalized_query, k)\n",
    "\n",
    "    retrieved_chunks = []\n",
    "    topk = []\n",
    "    references = []\n",
    "\n",
    "    for idx, distance in zip(indices[0], distances[0]):\n",
    "        chunk_info = chunk_doc_mapping[idx]\n",
    "        content = chunk_info['content']\n",
    "        doc_id = chunk_info['doc_id']\n",
    "        topk.append(doc_id)\n",
    "        references.append({\n",
    "            \"score\": float(distance),  # float32를 float으로 변환\n",
    "            \"content\": content\n",
    "        })\n",
    "        print(f\"검색된 Chunk ID: {chunk_info['chunk_id']}\")\n",
    "        print(f\"연관된 문서 ID: {doc_id}\")\n",
    "        print(f\"Chunk 내용: {content}\")\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "    return topk, references\n",
    "\n",
    "# LLM을 통한 RAG 시스템 구현\n",
    "def answer_question(messages):\n",
    "    # 함수 출력 초기화\n",
    "    response = {\"standalone_query\": \"\", \"topk\": [], \"references\": [], \"answer\": \"\"}\n",
    "    \n",
    "    # 메시지에서 검색 쿼리를 추출\n",
    "    query = messages[0]['content']\n",
    "\n",
    "    # standalone_query로 설정\n",
    "    response[\"standalone_query\"] = query\n",
    "\n",
    "    # 검색 결과 추출\n",
    "    topk, references = get_retrieved_chunks(query)\n",
    "\n",
    "    # 검색 결과를 response에 추가\n",
    "    response[\"topk\"] = topk\n",
    "    response[\"references\"] = references\n",
    "\n",
    "    # 임의로 생성된 답변 (LLM 통합 부분에 적용)\n",
    "    response[\"answer\"] = f\"{query}에 대한 결과는 위 문서들을 참조하세요.\"\n",
    "\n",
    "    return response\n",
    "\n",
    "# 평가를 위한 파일을 읽어서 각 평가 데이터에 대해서 결과 추출 후 파일에 저장\n",
    "def eval_rag(eval_filename, output_filename):\n",
    "    with open(eval_filename) as f, open(output_filename, \"w\") as of:\n",
    "        idx = 0\n",
    "        for line in f:\n",
    "            # if idx >= 3:  # 3번까지만 반복\n",
    "            #     break\n",
    "            j = json.loads(line)\n",
    "            print(f'Test {idx}\\nQuestion: {j[\"msg\"]}')\n",
    "            response = answer_question(j[\"msg\"])\n",
    "            print(f'Answer: {response[\"answer\"]}\\n')\n",
    "\n",
    "            # 대회 score 계산은 topk 정보를 사용, answer 정보는 LLM을 통한 자동평가시 활용\n",
    "            output = {\"eval_id\": j[\"eval_id\"], \"standalone_query\": response[\"standalone_query\"], \"topk\": response[\"topk\"], \"answer\": response[\"answer\"], \"references\": response[\"references\"]}\n",
    "            of.write(f'{json.dumps(output, ensure_ascii=False)}\\n')\n",
    "            idx += 1\n",
    "\n",
    "# 평가 데이터에 대해서 결과 생성 - 파일 포맷은 jsonl이지만 파일명은 csv 사용\n",
    "eval_rag(\"/ir/data/eval.jsonl\", \"/ir/sample_submission_2.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "cannot assign to expression here. Maybe you meant '==' instead of '='? (853420656.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[6], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    1+ 1 =daf\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m cannot assign to expression here. Maybe you meant '==' instead of '='?\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import json\n",
    "import numpy as np\n",
    "import faiss\n",
    "from openai import OpenAI  # Upstage API 사용\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# .env 파일 로드\n",
    "load_dotenv()\n",
    "\n",
    "# API_KEY 값을 가져옴\n",
    "openai_api_key = os.getenv(OPENAI_API_KEY)\n",
    "upstage_api_key = os.getenv(OPENAI_API_KEY)\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = openai_api_key\n",
    "\n",
    "def normalize(embeddings):\n",
    "    norms = np.linalg.norm(embeddings, axis=1, keepdims=True)\n",
    "    return (embeddings / norms).astype(np.float32)  # float32로 변환\n",
    "\n",
    "# Upstage API 클라이언트 설정\n",
    "client = OpenAI(\n",
    "    api_key= upstage_api_key,\n",
    "    base_url=\"https://api.upstage.ai/v1/solar\"\n",
    ")\n",
    "\n",
    "index = faiss.read_index(\"knn_index_cosine.faiss\")\n",
    "\n",
    "# 문서 ID 및 chunk ID 매핑 파일 로드\n",
    "with open(\"chunk_mappings.json\", \"r\") as f:\n",
    "    chunk_doc_mapping = json.load(f)\n",
    "\n",
    "# 쿼리 입력 후 Upstage API를 통해 임베딩 생성\n",
    "def get_retrieved_chunks(query, k=5):\n",
    "    query_result = client.embeddings.create(\n",
    "        model=\"solar-embedding-1-large-passage\",\n",
    "        input=query\n",
    "    ).data[0].embedding\n",
    "\n",
    "    query_embedding = np.array(query_result).reshape(1, -1)\n",
    "\n",
    "    # 쿼리 임베딩 정규화\n",
    "    normalized_query = normalize(query_embedding)\n",
    "\n",
    "    # FAISS 검색 실행\n",
    "    distances, indices = index.search(normalized_query, k)\n",
    "\n",
    "    retrieved_chunks = []\n",
    "    topk = []\n",
    "    references = []\n",
    "\n",
    "    for idx, distance in zip(indices[0], distances[0]):\n",
    "        chunk_info = chunk_doc_mapping[idx]\n",
    "        content = chunk_info['content']\n",
    "        doc_id = chunk_info['doc_id']\n",
    "        topk.append(doc_id)\n",
    "        references.append({\n",
    "            \"score\": float(distance),  # float32를 float으로 변환\n",
    "            \"content\": content\n",
    "        })\n",
    "        print(f\"검색된 Chunk ID: {chunk_info['chunk_id']}\")\n",
    "        print(f\"연관된 문서 ID: {doc_id}\")\n",
    "        print(f\"Chunk 내용: {content}\")\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "    return topk, references\n",
    "\n",
    "# LLM을 통한 질문 판별 함수\n",
    "def requires_retrieval(messages):\n",
    "    {\n",
    "    \"role\": \"system\",\n",
    "    \"content\": \"\"\"\n",
    "    당신은 과학적 질문과 비과학적 질문을 구분하는 전문가입니다. 사용자가 과학, 물리, 화학, 생물학, 의학 또는 기술과 관련된 질문을 하면, 검색이 필요하다고 표시하세요. 질문이 더 일반적인 내용이라면 직접 답변을 생성하세요.\n",
    "    \"\"\"\n",
    "    }\n",
    "    \n",
    "    # LLM에게 대화가 과학적인지 판단하도록 요청\n",
    "    result = client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo-1106\",\n",
    "        messages=[system_message] + messages,\n",
    "        temperature=0\n",
    "    )\n",
    "\n",
    "    # LLM이 판단한 결과 반환 (True: 검색 필요, False: 검색 불필요)\n",
    "    return \"retrieval\" in result.choices[0].message[\"content\"].lower()\n",
    "\n",
    "# 과학 상식 전문가 프롬프트\n",
    "persona_qa = \"\"\"\n",
    "## Role: 과학 상식 전문가\n",
    "\n",
    "## Instructions\n",
    "- 사용자의 이전 메시지 정보 및 주어진 Reference 정보를 활용하여 간결하게 답변을 생성한다.\n",
    "- 주어진 검색 결과 정보로 대답할 수 없는 경우는 정보가 부족해서 답을 할 수 없다고 대답한다.\n",
    "- 한국어로 답변을 생성한다.\n",
    "\"\"\"\n",
    "\n",
    "# 과학 상식 외 질문에 대한 프롬프트\n",
    "persona_function_calling = \"\"\"\n",
    "## Role: 과학 상식 전문가\n",
    "\n",
    "## Instruction\n",
    "- 사용자가 대화를 통해 과학 지식에 관한 주제로 질문하면 search api를 호출할 수 있어야 한다.\n",
    "- 과학 상식과 관련되지 않은 나머지 대화 메시지에는 적절한 대답을 생성한다.\n",
    "\"\"\"\n",
    "\n",
    "# LLM을 통한 RAG 시스템 구현\n",
    "def answer_question(messages):\n",
    "    # 함수 출력 초기화\n",
    "    response = {\"standalone_query\": \"\", \"topk\": [], \"references\": [], \"answer\": \"\"}\n",
    "\n",
    "    if requires_retrieval(messages):\n",
    "        # 과학 관련 질문의 경우\n",
    "        query = \" \".join([msg['content'] for msg in messages if msg['role'] == 'user'])\n",
    "        print(f\"Transformed Query for Retrieval: {query}\")\n",
    "\n",
    "        # standalone_query로 설정\n",
    "        response[\"standalone_query\"] = query\n",
    "\n",
    "        # 검색 결과 추출\n",
    "        topk, references = get_retrieved_chunks(query)\n",
    "\n",
    "        # 검색 결과를 response에 추가\n",
    "        response[\"topk\"] = topk\n",
    "        response[\"references\"] = references\n",
    "\n",
    "        # 검색된 문서에서 답변 생성 (프롬프트 적용)\n",
    "        content = \"\\n\".join([ref[\"content\"] for ref in references])\n",
    "        messages.append({\"role\": \"assistant\", \"content\": content})\n",
    "\n",
    "        result = client.chat.completions.create(\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            messages=messages,\n",
    "            temperature=0\n",
    "        )\n",
    "        response[\"answer\"] = result.choices[0].message[\"content\"]\n",
    "    \n",
    "    else:\n",
    "        # 과학과 관련 없는 질문에 대해 LLM이 직접 답변 생성\n",
    "        messages.append({\"role\": \"system\", \"content\": persona_function_calling})\n",
    "\n",
    "        result = client.chat.completions.create(\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            messages=messages,\n",
    "            temperature=0\n",
    "        )\n",
    "        response[\"answer\"] = result.choices[0].message[\"content\"]\n",
    "\n",
    "    return response\n",
    "\n",
    "# 평가를 위한 파일을 읽어서 각 평가 데이터에 대해서 결과 추출 후 파일에 저장\n",
    "def eval_rag(eval_filename, output_filename):\n",
    "    with open(eval_filename) as f, open(output_filename, \"w\") as of:\n",
    "        idx = 0\n",
    "        for line in f:\n",
    "            if idx >= 3:  # 3번까지만 반복\n",
    "                break\n",
    "            j = json.loads(line)\n",
    "            print(f'Test {idx}\\nQuestion: {j[\"msg\"]}')\n",
    "            response = answer_question(j[\"msg\"])\n",
    "            print(f'Answer: {response[\"answer\"]}\\n')\n",
    "\n",
    "            # 대회 score 계산은 topk 정보를 사용, answer 정보는 LLM을 통한 자동평가시 활용\n",
    "            output = {\"eval_id\": j[\"eval_id\"], \"standalone_query\": response[\"standalone_query\"], \"topk\": response[\"topk\"], \"answer\": response[\"answer\"], \"references\": response[\"references\"]}\n",
    "            of.write(f'{json.dumps(output, ensure_ascii=False)}\\n')\n",
    "            idx += 1\n",
    "\n",
    "# 평가 데이터에 대해서 결과 생성 - 파일 포맷은 jsonl이지만 파일명은 csv 사용\n",
    "eval_rag(\"/ir/data/eval.jsonl\", \"/ir/sample_submission.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "cannot assign to expression here. Maybe you meant '==' instead of '='? (2303401404.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[7], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    1+ 1 =daf\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m cannot assign to expression here. Maybe you meant '==' instead of '='?\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import json\n",
    "import numpy as np\n",
    "import faiss\n",
    "from openai import OpenAI  # Upstage API 사용\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# .env 파일 로드\n",
    "load_dotenv('/upstage-ai-advanced-ir7/.env')\n",
    "\n",
    "# API_KEY 값을 가져옴\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "upstage_api_key = os.getenv('UPSTAGE_API_KEY')\n",
    "\n",
    "\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = openai_api_key\n",
    "\n",
    "# Upstage API 클라이언트 설정\n",
    "client = OpenAI(\n",
    "    api_key= upstage_api_key,\n",
    "    base_url=\"https://api.upstage.ai/v1/solar\"\n",
    ")\n",
    "\n",
    "def normalize(embeddings):\n",
    "    norms = np.linalg.norm(embeddings, axis=1, keepdims=True)\n",
    "    return (embeddings / norms).astype(np.float32)  # float32로 변환\n",
    "\n",
    "# Upstage API 클라이언트 설정\n",
    "\n",
    "index = faiss.read_index(\"knn_index_cosine.faiss\")\n",
    "\n",
    "# 문서 ID 및 chunk ID 매핑 파일 로드\n",
    "with open(\"chunk_mappings.json\", \"r\") as f:\n",
    "    chunk_doc_mapping = json.load(f)\n",
    "\n",
    "# 쿼리 입력 후 Upstage API를 통해 임베딩 생성\n",
    "def get_retrieved_chunks(query, k=5):\n",
    "    query_result = client.embeddings.create(\n",
    "        model=\"solar-embedding-1-large-passage\",\n",
    "        input=query\n",
    "    ).data[0].embedding\n",
    "\n",
    "    query_embedding = np.array(query_result).reshape(1, -1)\n",
    "\n",
    "    # 쿼리 임베딩 정규화\n",
    "    normalized_query = normalize(query_embedding)\n",
    "\n",
    "    # FAISS 검색 실행\n",
    "    distances, indices = index.search(normalized_query, k)\n",
    "\n",
    "    retrieved_chunks = []\n",
    "    topk = []\n",
    "    references = []\n",
    "\n",
    "    for idx, distance in zip(indices[0], distances[0]):\n",
    "        chunk_info = chunk_doc_mapping[idx]\n",
    "        content = chunk_info['content']\n",
    "        doc_id = chunk_info['doc_id']\n",
    "        topk.append(doc_id)\n",
    "        references.append({\n",
    "            \"score\": float(distance),  # float32를 float으로 변환\n",
    "            \"content\": content\n",
    "        })\n",
    "        print(f\"검색된 Chunk ID: {chunk_info['chunk_id']}\")\n",
    "        print(f\"연관된 문서 ID: {doc_id}\")\n",
    "        print(f\"Chunk 내용: {content}\")\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "    return topk, references\n",
    "\n",
    "# LLM을 통한 질문 판별 함수\n",
    "def requires_retrieval(messages):\n",
    "    # LLM에게 과학적인 질문인지 판단하도록 요청\n",
    "    system_message = {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"\"\"\n",
    "        당신은 과학적 질문과 비과학적 질문을 구분하는 전문가입니다. 사용자가 과학, 물리, 화학, 생물학, 의학 또는 기술과 관련된 질문을 하면, 검색이 필요하다고 표시하세요. 질문이 더 일반적인 내용이라면 직접 답변을 생성하세요.\n",
    "        \"\"\"\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        result = client.chat.completions.create(\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            messages=[system_message] + messages,\n",
    "            temperature=0\n",
    "        )\n",
    "    except openai.error.OpenAIError as e:\n",
    "        print(f\"OpenAI API error: {e}\")\n",
    "        return response\n",
    "\n",
    "    # LLM이 판단한 결과 반환 (True: 검색 필요, False: 검색 불필요)\n",
    "    return \"retrieval\" in result.choices[0].message[\"content\"].lower()\n",
    "\n",
    "# 과학 상식 전문가 프롬프트\n",
    "persona_qa = \"\"\"\n",
    "## Role: 과학 상식 전문가\n",
    "\n",
    "## Instructions\n",
    "- 사용자의 이전 메시지 정보 및 주어진 Reference 정보를 활용하여 간결하게 답변을 생성한다.\n",
    "- 주어진 검색 결과 정보로 대답할 수 없는 경우는 정보가 부족해서 답을 할 수 없다고 대답한다.\n",
    "- 한국어로 답변을 생성한다.\n",
    "\"\"\"\n",
    "\n",
    "# 과학 상식 외 질문에 대한 프롬프트\n",
    "persona_function_calling = \"\"\"\n",
    "## Role: 과학 상식 전문가\n",
    "\n",
    "## Instruction\n",
    "- 사용자가 대화를 통해 과학 지식에 관한 주제로 질문하면 search api를 호출할 수 있어야 한다.\n",
    "- 과학 상식과 관련되지 않은 나머지 대화 메시지에는 적절한 대답을 생성한다.\n",
    "\"\"\"\n",
    "\n",
    "# LLM을 통한 RAG 시스템 구현\n",
    "def answer_question(messages):\n",
    "    # 함수 출력 초기화\n",
    "    response = {\"standalone_query\": \"\", \"topk\": [], \"references\": [], \"answer\": \"\"}\n",
    "\n",
    "    if requires_retrieval(messages):\n",
    "        # 과학 관련 질문의 경우\n",
    "        query = \" \".join([msg['content'] for msg in messages if msg['role'] == 'user'])\n",
    "        print(f\"Transformed Query for Retrieval: {query}\")\n",
    "\n",
    "        # standalone_query로 설정\n",
    "        response[\"standalone_query\"] = query\n",
    "\n",
    "        # 검색 결과 추출\n",
    "        topk, references = get_retrieved_chunks(query)\n",
    "\n",
    "        # 검색 결과를 response에 추가\n",
    "        response[\"topk\"] = topk\n",
    "        response[\"references\"] = references\n",
    "\n",
    "        # 검색된 문서에서 답변 생성 (프롬프트 적용)\n",
    "        content = \"\\n\".join([ref[\"content\"] for ref in references])\n",
    "        messages.append({\"role\": \"assistant\", \"content\": content})\n",
    "\n",
    "        result = client.chat.completions.create(\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            messages=messages,\n",
    "            temperature=0\n",
    "        )\n",
    "        response[\"answer\"] = result.choices[0].message[\"content\"]\n",
    "    \n",
    "    else:\n",
    "        # 과학과 관련 없는 질문에 대해 LLM이 직접 답변 생성\n",
    "        messages.append({\"role\": \"system\", \"content\": persona_function_calling})\n",
    "\n",
    "        result = client.chat.completions.create(\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            messages=messages,\n",
    "            temperature=0\n",
    "        )\n",
    "        response[\"answer\"] = result.choices[0].message[\"content\"]\n",
    "\n",
    "    return response\n",
    "\n",
    "# 평가를 위한 파일을 읽어서 각 평가 데이터에 대해서 결과 추출 후 파일에 저장\n",
    "def eval_rag(eval_filename, output_filename):\n",
    "    with open(eval_filename) as f, open(output_filename, \"w\") as of:\n",
    "        idx = 0\n",
    "        for line in f:\n",
    "            if idx >= 3:  # 3번까지만 반복\n",
    "                break\n",
    "            j = json.loads(line)\n",
    "            print(f'Test {idx}\\nQuestion: {j[\"msg\"]}')\n",
    "            response = answer_question(j[\"msg\"])\n",
    "            print(f'Answer: {response[\"answer\"]}\\n')\n",
    "\n",
    "            # 대회 score 계산은 topk 정보를 사용, answer 정보는 LLM을 통한 자동평가시 활용\n",
    "            output = {\"eval_id\": j[\"eval_id\"], \"standalone_query\": response[\"standalone_query\"], \"topk\": response[\"topk\"], \"answer\": response[\"answer\"], \"references\": response[\"references\"]}\n",
    "            of.write(f'{json.dumps(output, ensure_ascii=False)}\\n')\n",
    "            idx += 1\n",
    "\n",
    "# 평가 데이터에 대해서 결과 생성 - 파일 포맷은 jsonl이지만 파일명은 csv 사용\n",
    "eval_rag(\"/ir/data/eval.jsonl\", \"/ir/sample_submission.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "import json\n",
    "import numpy as np\n",
    "import faiss\n",
    "from openai import OpenAI  # Upstage API 사용\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "cannot assign to expression here. Maybe you meant '==' instead of '='? (3474105294.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[9], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    1+ 1 =daf\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m cannot assign to expression here. Maybe you meant '==' instead of '='?\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# .env 파일 로드\n",
    "load_dotenv('/upstage-ai-advanced-ir7/.env')\n",
    "\n",
    "# API_KEY 값을 가져옴\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "upstage_api_key = os.getenv('UPSTAGE_API_KEY')\n",
    "\n",
    "\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = openai_api_key\n",
    "\n",
    "# Upstage API 클라이언트 설정\n",
    "client = OpenAI(\n",
    "    api_key= upstage_api_key,\n",
    "    base_url=\"https://api.upstage.ai/v1/solar\"\n",
    ")\n",
    "os.environ[\"OPENAI_API_KEY\"] = openai_api_key\n",
    "llm_model = \"gpt-3.5-turbo-1106\"\n",
    "client = OpenAI()\n",
    "\n",
    "# openai.api_key = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "cannot assign to expression here. Maybe you meant '==' instead of '='? (1256807387.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[10], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    1+ 1 =daf\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m cannot assign to expression here. Maybe you meant '==' instead of '='?\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def transform_query_with_llm(messages):\n",
    "    \"\"\"\n",
    "    LLM을 사용하여 사용자의 여러 메시지를 기반으로 검색에 적합한 단일 쿼리 생성.\n",
    "    \"\"\"\n",
    "    # 시스템 메시지로 LLM에게 과제 부여 (한국어로)\n",
    "    system_message = {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"\"\"\n",
    "        당신은 사용자의 여러 대화 메시지를 하나의 검색 쿼리로 변환하는 전문가입니다. \n",
    "        사용자의 대화 중 중요한 정보만을 추출하여 간결하고 명확한 검색용 쿼리로 변환하세요. \n",
    "        대화 형식이 아닌, 연구나 조사를 위한 검색어처럼 정확한 질문을 생성하세요.\n",
    "        \"\"\"\n",
    "    }\n",
    "\n",
    "    # 사용자 메시지 준비\n",
    "    dialogue_messages = [{\"role\": msg['role'], \"content\": msg['content']} for msg in messages]\n",
    "\n",
    "    # LLM 호출을 위한 메시지 배열 생성\n",
    "    full_message = [system_message] + dialogue_messages\n",
    "\n",
    "    # OpenAI API 호출하여 적절한 검색 쿼리 생성\n",
    "    result = client.chat.completions.create(\n",
    "        model=\"gpt-4o\",  # LLM 모델 지정\n",
    "        messages=full_message,\n",
    "        temperature=0\n",
    "    )\n",
    "\n",
    "    # LLM이 생성한 쿼리 반환 (ChatCompletionMessage 형식에서 content에 직접 접근)\n",
    "    transformed_query = result.choices[0].message.content\n",
    "    print(f\"변환된 쿼리: {transformed_query}\")\n",
    "    return transformed_query\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"기억 상실증 걸리면 너무 무섭겠다.\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"네 맞습니다.\"},\n",
    "    {\"role\": \"user\", \"content\": \"원인이 뭘까.\"}\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "{\"role\": \"user\", \"content\": \"세균이 나쁜줄말 알았는데 그게 아니야?\"},\n",
    "{\"role\": \"assistant\", \"content\": \"네 맞습니다.\"},\n",
    "{\"role\": \"user\", \"content\": \"그럼 순기능에 대해 알려줄래?\"}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    \n",
    "{\"role\": \"user\", \"content\": \"은하에는 엄청나게 많은 별들이 모여 있잖아?\"}, {\"role\": \"assistant\", \"content\": \"네 맞습니다.\"}, {\"role\": \"user\", \"content\": \"그 많은 별들이 어떻게 뭉쳐 있을 수 있지?\"}\n",
    "\n",
    "\n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "변환된 쿼리: 은하 내 별들이 중력으로 어떻게 모여 있는가?\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'은하 내 별들이 중력으로 어떻게 모여 있는가?'"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transform_query_with_llm(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def science_query_detection(message):\n",
    "    \"\"\"\n",
    "    LLM을 사용하여 단일 사용자의 질문을 검색에 적합한 쿼리로 변환.\n",
    "    질문이 과학과 관련되어 있으면 '과학 관련 질문입니다'라고 답하고, \n",
    "    그렇지 않으면 '과학 관련 질문이 아닙니다'라고 답변.\n",
    "    \"\"\"\n",
    "    # 시스템 메시지로 LLM에게 과제 부여 (한국어로)\n",
    "    system_message = {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"\"\"\n",
    "        당신은 과학적 질문과 비과학적 질문을 구분하는 전문가입니다. \n",
    "        질문이 과학, 물리, 화학, 생물학, 의학 또는 기술과 관련되어 있으면 '과학 관련 질문입니다'라고 답하세요. \n",
    "        그렇지 않으면 '과학 관련 질문이 아닙니다'라고 답하세요. \n",
    "        과학 관련 질문일 경우에는 좀 더 명확하고 간결한 검색용 질문으로 변환하세요.\n",
    "        \"\"\"\n",
    "    }\n",
    "\n",
    "    # 사용자 질문 준비\n",
    "    user_message = {\"role\": message['role'], \"content\": message['content']}\n",
    "\n",
    "    # LLM 호출을 위한 전체 메시지 배열 생성\n",
    "    full_message = [system_message, user_message]\n",
    "\n",
    "    # OpenAI API 호출하여 적절한 검색 쿼리 생성\n",
    "    result = client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",  # LLM 모델 지정\n",
    "        messages=full_message,\n",
    "        temperature=0\n",
    "    )\n",
    "\n",
    "    # LLM이 생성한 쿼리 반환 (ChatCompletionMessage 형식에서 content에 직접 접근)\n",
    "    llm_response = result['choices'][0]['message']['content']\n",
    "\n",
    "    # 과학 관련 질문 여부 판단 및 처리\n",
    "    if \"과학 관련 질문이 아닙니다\" in llm_response:\n",
    "        print(\"과학 관련 질문이 아닙니다.\")\n",
    "        return \"과학 관련 질문이 아닙니다.\"\n",
    "    else:\n",
    "        # 과학 관련 질문일 경우 변환된 쿼리 반환\n",
    "        refined_query = llm_response.strip()\n",
    "        print(f\"변환된 쿼리: {refined_query}\")\n",
    "        return refined_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "message = [\n",
    "    {\"role\": \"user\", \"content\": \"바다가 육지의 날씨에 미치는 영향은?\"},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not str",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[100], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mscience_query_detection\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessage\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[97], line 19\u001b[0m, in \u001b[0;36mscience_query_detection\u001b[0;34m(message)\u001b[0m\n\u001b[1;32m      8\u001b[0m system_message \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msystem\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;124m    \u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m     16\u001b[0m }\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# 사용자 질문 준비\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m user_message \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[43mmessage\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrole\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: message[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m'\u001b[39m]}\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# LLM 호출을 위한 전체 메시지 배열 생성\u001b[39;00m\n\u001b[1;32m     22\u001b[0m full_message \u001b[38;5;241m=\u001b[39m [system_message, user_message]\n",
      "\u001b[0;31mTypeError\u001b[0m: list indices must be integers or slices, not str"
     ]
    }
   ],
   "source": [
    "science_query_detection(message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def science_query_detection(message):\n",
    "    \"\"\"\n",
    "    LLM을 사용하여 사용자의 질문이 과학과 관련된지 여부를 판단하고, \n",
    "    관련된 경우 검색용으로 적합한 질문으로 변환.\n",
    "    질문이 과학과 관련되어 있으면 '과학 관련 질문입니다'라고 답하고, \n",
    "    그렇지 않으면 '과학 관련 질문이 아닙니다'라고 답변.\n",
    "    \"\"\"\n",
    "    # 시스템 메시지로 LLM에게 과제 부여 (한국어로)\n",
    "    system_message = {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"\"\"\n",
    "        당신은 과학적 질문과 비과학적 질문을 구분하는 전문가입니다. \n",
    "        질문이 과학, 물리, 화학, 생물학, 의학, 영양학, 인간의성, 또는 기술과 관련되어 있으면 '과학 관련 질문입니다'라고 답하세요. \n",
    "        그렇지 않으면 '과학 관련 질문이 아닙니다'라고 답하세요. \n",
    "        과학 관련 질문일 경우에는 좀 더 명확하고 간결한 검색용 질문으로 변환하세요.\n",
    "        \"\"\"\n",
    "    }\n",
    "\n",
    "    # 단일 질문을 message로 받아 처리\n",
    "    user_message = {\"role\": message[0]['role'], \"content\": message[0]['content']}\n",
    "\n",
    "    # LLM 호출을 위한 전체 메시지 배열 생성\n",
    "    full_message = [system_message, user_message]\n",
    "\n",
    "    # OpenAI API 호출하여 적절한 검색 쿼리 생성\n",
    "    result = client.chat.completions.create(\n",
    "        model=\"gpt-4o\",  # LLM 모델 지정\n",
    "        messages=full_message,\n",
    "        temperature=0\n",
    "    )\n",
    "\n",
    "    # LLM이 생성한 응답을 반환\n",
    "    llm_response = result['choices'][0]['message']['content']\n",
    "\n",
    "    # 과학 관련 질문 여부 판단 및 처리\n",
    "    if \"과학 관련 질문이 아닙니다\" in llm_response:\n",
    "        print(\"과학 관련 질문이 아닙니다.\")\n",
    "        return \"과학 관련 질문이 아닙니다.\"\n",
    "    else:\n",
    "        # 과학 관련 질문일 경우 변환된 쿼리 반환\n",
    "        refined_query = llm_response.strip()\n",
    "        print(f\"변환된 쿼리: {refined_query}\")\n",
    "        return refined_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "def science_query_detection(message):\n",
    "    \"\"\"\n",
    "    LLM을 사용하여 단일 사용자의 질문을 검색에 적합한 쿼리로 변환.\n",
    "    질문이 과학과 관련되어 있으면 '과학 관련 질문입니다'라고 답하고, \n",
    "    그렇지 않으면 '과학 관련 질문이 아닙니다'라고 답변.\n",
    "    \"\"\"\n",
    "    # 시스템 메시지로 LLM에게 과제 부여 (한국어로)\n",
    "    system_message = {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"\"\"\n",
    "        당신은 과학적 질문과 비과학적 질문을 구분하는 전문가입니다. \n",
    "        질문이 물리학, 화학, 생물학, 임상 의학, 바이러스학, 인간 노화, 컴퓨터 공학, 해부학, 유전학, 전기 공학, 천문학, 세계 상식 또는 기술과 관련되지 않으면 '과학 관련 질문이 아닙니다'라고 답하세요.\n",
    "        과학 관련 질문일 경우에는 자료 검색에 적합하도록 명확하고 간결한 질문으로 변환하세요.\n",
    "        \"\"\"\n",
    "    }\n",
    "\n",
    "    # 사용자 질문 준비\n",
    "    user_message = {\"role\": message[0]['role'], \"content\": message[0]['content']}\n",
    "\n",
    "    # LLM 호출을 위한 전체 메시지 배열 생성\n",
    "    full_message = [system_message, user_message]\n",
    "\n",
    "    # OpenAI API 호출하여 적절한 검색 쿼리 생성\n",
    "    result = client.chat.completions.create(\n",
    "        model=\"gpt-4o\",  # LLM 모델 지정\n",
    "        messages=full_message,\n",
    "        temperature=0\n",
    "    )\n",
    "\n",
    "    # LLM이 생성한 응답을 반환\n",
    "    llm_response = result.choices[0].message.content\n",
    "\n",
    "    # 과학 관련 질문 여부 판단 및 처리\n",
    "    if \"과학 관련 질문이 아닙니다\" in llm_response:\n",
    "        print(\"과학 관련 질문이 아닙니다.\")\n",
    "        return \"과학 관련 질문이 아닙니다.\"\n",
    "    else:\n",
    "        # 과학 관련 질문일 경우 변환된 쿼리 반환\n",
    "        refined_query = llm_response.strip()\n",
    "        print(f\"변환된 쿼리: {refined_query}\")\n",
    "        return refined_query\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "과학 관련 질문이 아닙니다.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'과학 관련 질문이 아닙니다.'"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "message = [\n",
    "    {\"role\": \"user\", \"content\": \"나 과학 공부하기 싫어\"}\n",
    "]\n",
    "\n",
    "science_query_detection(message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def question_maker(document):\n",
    "    \"\"\"\n",
    "    LLM을 사용하여 문서에서 질문을 만들어낸다. 문서가 상식적이거나 논리적이지 않으면 말이 되지 않는다고 말한다.\n",
    "    \"\"\"\n",
    "    # 시스템 메시지로 LLM에게 과제 부여 (한국어로)\n",
    "    system_message = {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"\"\"\n",
    "        당신은 과학적 문서를 보고 질문을 만들어 내는 전문가 입니다\n",
    "        질문이 상식적이지 않거나 논리적이지 않으면 말이 되지 않습니다. 라고 말하고\n",
    "        아니면 이 문서에서 답을 찾을 수 있을만한 질문 5개를 만들어 내시오.\n",
    "        \"\"\"\n",
    "    }\n",
    "\n",
    "    # 사용자 질문 준비\n",
    "    document = {\"src\": document[0]['src'], \"content\": document[0]['content']}\n",
    "\n",
    "    # LLM 호출을 위한 전체 메시지 배열 생성\n",
    "    full_message = [system_message, user_message]\n",
    "\n",
    "    # OpenAI API 호출하여 적절한 검색 쿼리 생성\n",
    "    result = client.chat.completions.create(\n",
    "        model=\"gpt-4o\",  # LLM 모델 지정\n",
    "        messages=full_message,\n",
    "        temperature=0\n",
    "    )\n",
    "\n",
    "    # LLM이 생성한 응답을 반환\n",
    "    llm_response = result.choices[0].document.content\n",
    "\n",
    "    # 과학 관련 질문 여부 판단 및 처리\n",
    "    if \"과학 관련 질문이 아닙니다\" in llm_response:\n",
    "        print(\"과학 관련 질문이 아닙니다.\")\n",
    "        return \"과학 관련 질문이 아닙니다.\"\n",
    "    else:\n",
    "        # 과학 관련 질문일 경우 변환된 쿼리 반환\n",
    "        refined_query = llm_response.strip()\n",
    "        print(f\"변환된 쿼리: {refined_query}\")\n",
    "        return refined_query\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def question_maker(document):\n",
    "#     \"\"\"\n",
    "#     LLM을 사용하여 문서에서 논리적이고 상식적인 질문 5개를 생성합니다.\n",
    "#     문서가 논리적이지 않거나 상식적이지 않으면 \"말이 되지 않습니다.\"라는 응답을 반환합니다.\n",
    "#     \"\"\"\n",
    "#     # 시스템 메시지로 LLM에게 과제 부여 (한국어로)\n",
    "#     system_message = {\n",
    "#         \"role\": \"system\",\n",
    "#         \"content\": \"\"\"\n",
    "#         당신은 과학적 문서를 보고 논리적이고 상식적인 질문을 만들어내는 전문가입니다.\n",
    "#         문서가 비논리적이거나 상식적이지 않다면 \"말이 되지 않습니다.\"라고 답변하세요.\n",
    "#         그렇지 않다면, 이 문서에서 답을 찾을 수 있을만한 5개의 질문을 논리적이고 명확하게 작성하세요.\n",
    "#         \"\"\"\n",
    "#     }\n",
    "\n",
    "#     # 문서의 src와 content를 포함한 메시지 준비\n",
    "#     document_message = {\"role\": \"user\", \"content\": f\"문서 출처: {document[0]['src']}\\n내용: {document[0]['content']}\"}\n",
    "\n",
    "#     # LLM 호출을 위한 전체 메시지 배열 생성\n",
    "#     full_message = [system_message, document_message]\n",
    "\n",
    "#     # OpenAI API 호출하여 적절한 질문 생성\n",
    "#     result = client.chat.completions.create(\n",
    "#         model=\"gpt-4\",  # 사용할 모델 지정\n",
    "#         messages=full_message,\n",
    "#         temperature=0\n",
    "#     )\n",
    "\n",
    "#     # LLM이 생성한 응답을 가져오기\n",
    "#     llm_response = result.choices[0].message[\"content\"]\n",
    "\n",
    "#     # 과학적 질문을 반환하거나, 문서가 비논리적인 경우 처리\n",
    "#     if \"말이 되지 않습니다\" in llm_response:\n",
    "#         print(\"문서가 논리적이지 않습니다.\")\n",
    "#         return \"말이 되지 않습니다.\"\n",
    "#     else:\n",
    "#         # 논리적인 질문 5개를 반환\n",
    "#         questions = llm_response.strip().split(\"\\n\")\n",
    "#         print(f\"생성된 질문들: {questions}\")\n",
    "#         return questions\n",
    "\n",
    "\n",
    "# def question_maker(document):\n",
    "#     \"\"\"\n",
    "#     LLM을 사용하여 문서에서 논리적이고 상식적인 질문 5개를 생성합니다.\n",
    "#     문서가 논리적이지 않거나 상식적이지 않으면 \"말이 되지 않습니다.\"라는 응답을 반환합니다.\n",
    "#     \"\"\"\n",
    "#     # 시스템 메시지로 LLM에게 과제 부여 (한국어로)\n",
    "#     system_message = {\n",
    "#         \"role\": \"system\",\n",
    "#         \"content\": \"\"\"\n",
    "#         당신은 과학적 문서를 분석하여 그 문서에서 논리적이고 상식적인 질문을 만드는 전문가입니다.\n",
    "#         문서가 비논리적이거나 상식적이지 않다면, \"이 문서는 논리적이지 않습니다.\"라고 답변하세요.\n",
    "#         논리적이고 상식적인 문서라면, 그 문서에서 답을 찾을 수 있을 만한 명확하고 중요한 5개의 질문을 생성하세요.\n",
    "#         질문은 반드시 문서의 중요한 개념이나 정보를 기반으로 하며, 과학적 사실이나 원리를 반영해야 합니다.\n",
    "#         질문은 \"왜\", \"어떻게\", \"무엇을\" 등으로 시작하는 형태로 구성해야 합니다.\n",
    "#         질문을 나열할 때는 번호를 매겨 명확히 구분하고, 답을 유도하지 않도록 주의하세요.\n",
    "#         \"\"\"\n",
    "#     }\n",
    "\n",
    "#     # 문서의 src와 content를 포함한 메시지 준비\n",
    "#     document_message = {\"role\": \"user\", \"content\": f\"문서 출처: {document[0]['src']}\\n내용: {document[0]['content']}\"}\n",
    "\n",
    "#     # LLM 호출을 위한 전체 메시지 배열 생성\n",
    "#     full_message = [system_message, document_message]\n",
    "\n",
    "#     # OpenAI API 호출하여 적절한 질문 생성\n",
    "#     result = client.chat.completions.create(\n",
    "#         model=\"gpt-4o\",  # 사용할 모델 지정\n",
    "#         messages=full_message,\n",
    "#         temperature=0\n",
    "#     )\n",
    "\n",
    "#     # LLM이 생성한 응답을 가져오기\n",
    "#     llm_response = result.choices[0].message.content\n",
    "\n",
    "#     # 문서가 비논리적인 경우 처리\n",
    "#     if \"이 문서는 논리적이지 않습니다\" in llm_response:\n",
    "#         print(\"문서가 논리적이지 않습니다.\")\n",
    "#         return \"이 문서는 논리적이지 않습니다.\"\n",
    "#     else:\n",
    "#         # 논리적인 질문 5개를 반환\n",
    "#         questions = llm_response.strip().split(\"\\n\")\n",
    "#         print(f\"생성된 질문들: {questions}\")\n",
    "#         return questions\n",
    "    \n",
    "    \n",
    "def question_maker(document):\n",
    "    \"\"\"\n",
    "    LLM을 사용하여 문서에서 논리적 오류와 상식적 타당성을 평가하고, \n",
    "    두 가지 조건이 모두 충족될 경우 논리적이고 상식적인 질문 5개를 생성합니다.\n",
    "    \"\"\"\n",
    "    # 시스템 메시지로 LLM에게 과제 부여 (한국어로)\n",
    "    system_message = {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"\"\"\n",
    "        당신은 과학적 문서를 평가하는 전문가입니다.\n",
    "\n",
    "        **1. 논리적 오류 평가**:\n",
    "            - 문서의 설명에서 과학적 원리와 모순되거나 자명한 논리적 오류가 있는지 확인하세요.\n",
    "            - 예를 들어, '해머로 철벽을 치는 것이 열을 발생시키기 위한 목적이다'라는 설명은, 과학적 원리(열역학)와는 일치할지 모르지만 그 본래 목적과는 어긋납니다.\n",
    "\n",
    "        **2. 상식적 타당성 평가**:\n",
    "            - 설명이 과학적으로 맞다고 해도 상식적으로 그 설명이 현실적인지 판단하세요.\n",
    "            - 예를 들어, '가스레인지를 이용해 집안을 따뜻하게 한다'는 설명은 상식적이지 않으며, 주된 용도와는 부합하지 않습니다.\n",
    "\n",
    "        **결과**:\n",
    "            - 두 가지 조건이 모두 충족되지 않으면 \"이 문서는 논리적 오류가 있으며, 상식적이지 않습니다.\"라고 답변하세요.\n",
    "            - 두 가지 조건이 모두 충족되면, 이 문서에서 답을 찾을 수 있을 만한 논리적이고 상식적인 질문 5개를 생성하세요.\n",
    "        \"\"\"\n",
    "    }\n",
    "\n",
    "    # 사용자 문서 입력 준비\n",
    "    document_message = {\"role\": \"user\", \"content\": f\"문서 출처: {document[0]['src']}\\n내용: {document[0]['content']}\"}\n",
    "\n",
    "    # LLM 호출을 위한 전체 메시지 배열 생성\n",
    "    full_message = [system_message, document_message]\n",
    "\n",
    "    # OpenAI API 호출하여 적절한 질문 생성\n",
    "    result = client.chat.completions.create(\n",
    "        model=\"gpt-4o\",  # 사용할 모델 지정\n",
    "        messages=full_message,\n",
    "        temperature=0\n",
    "    )\n",
    "\n",
    "    # LLM이 생성한 응답을 가져오기\n",
    "    llm_response = result.choices[0].message.content\n",
    "\n",
    "    # 논리적 오류와 상식적 타당성을 반환하거나, 논리적인 질문 5개를 반환\n",
    "    if \"논리적 오류가 있으며, 상식적이지 않습니다\" in llm_response:\n",
    "        print(\"문서가 논리적 오류가 있으며, 상식적이지 않습니다.\")\n",
    "        return \"이 문서는 논리적 오류가 있으며, 상식적이지 않습니다.\"\n",
    "    else:\n",
    "        # 논리적인 질문 5개를 반환\n",
    "        questions = llm_response.strip().split(\"\\n\")\n",
    "        print(f\"생성된 질문들: {questions}\")\n",
    "        return questions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "{\"docid\": \"82b095fd-2fb6-48ae-8476-2f457f8b6650\", \"src\": \"ko_ai2_arc__ARC_Challenge__train\", \"content\": \"인간 남성과 여성의 생식세포는 같은 수의 염색체를 가지고 있습니다. 남성은 23쌍의 염색체를 가지고 있으며, 여성도 23쌍의 염색체를 가지고 있습니다. 따라서, 일반적인 남성과 여성의 생식세포가 결합하여 자손을 생산할 때, 염색체의 수는 두 배가 됩니다. 이는 염색체의 유전 정보를 조합하여 새로운 개체를 형성하는 과정입니다. 염색체는 DNA를 포함하고 있으며, DNA는 유전 정보를 담고 있는 분자입니다. 따라서, 염색체의 수가 두 배가 되면, 자손은 부모의 유전 정보를 조합하여 새로운 개체를 형성하게 됩니다.\"}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "문서가 논리적 오류가 있으며, 상식적이지 않습니다.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'이 문서는 논리적 오류가 있으며, 상식적이지 않습니다.'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document = [\n",
    "    \n",
    "{\"docid\": \"82b095fd-2fb6-48ae-8476-2f457f8b6650\", \"src\": \"ko_ai2_arc__ARC_Challenge__train\", \"content\": \"인간 남성과 여성의 생식세포는 같은 수의 염색체를 가지고 있습니다. 남성은 23쌍의 염색체를 가지고 있으며, 여성도 23쌍의 염색체를 가지고 있습니다. 따라서, 일반적인 남성과 여성의 생식세포가 결합하여 자손을 생산할 때, 염색체의 수는 두 배가 됩니다. 이는 염색체의 유전 정보를 조합하여 새로운 개체를 형성하는 과정입니다. 염색체는 DNA를 포함하고 있으며, DNA는 유전 정보를 담고 있는 분자입니다. 따라서, 염색체의 수가 두 배가 되면, 자손은 부모의 유전 정보를 조합하여 새로운 개체를 형성하게 됩니다.\"}\n",
    "\n",
    "]\n",
    "\n",
    "question_maker(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ko_mmlu__nutrition__test'"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'role': 'user',\n",
       " 'content': '문서 출처: ko_mmlu__nutrition__test\\n내용: 건강한 사람이 에너지 균형을 평형 상태로 유지하는 것은 중요합니다. 에너지 균형은 에너지 섭취와 에너지 소비의 수학적 동등성을 의미합니다. 일반적으로 건강한 사람은 1-2주의 기간 동안 에너지 균형을 달성합니다. 이 기간 동안에는 올바른 식단과 적절한 운동을 통해 에너지 섭취와 에너지 소비를 조절해야 합니다. 식단은 영양가 있는 식품을 포함하고, 적절한 칼로리를 섭취해야 합니다. 또한, 운동은 에너지 소비를 촉진시키고 근육을 강화시킵니다. 이렇게 에너지 균형을 유지하면 건강을 유지하고 비만이나 영양 실조와 같은 문제를 예방할 수 있습니다. 따라서 건강한 사람은 에너지 균형을 평형 상태로 유지하는 것이 중요하며, 이를 위해 1-2주의 기간 동안 식단과 운동을 조절해야 합니다.'}"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document_message = {\"role\": \"user\", \"content\": f\"문서 출처: {document[0]['src']}\\n내용: {document[0]['content']}\"}\n",
    "document_message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'role': 'user',\n",
       " 'content': '문서 출처: ko_mmlu__nutrition__test\\n내용: 건강한 사람이 에너지 균형을 평형 상태로 유지하는 것은 중요합니다. 에너지 균형은 에너지 섭취와 에너지 소비의 수학적 동등성을 의미합니다. 일반적으로 건강한 사람은 1-2주의 기간 동안 에너지 균형을 달성합니다. 이 기간 동안에는 올바른 식단과 적절한 운동을 통해 에너지 섭취와 에너지 소비를 조절해야 합니다. 식단은 영양가 있는 식품을 포함하고, 적절한 칼로리를 섭취해야 합니다. 또한, 운동은 에너지 소비를 촉진시키고 근육을 강화시킵니다. 이렇게 에너지 균형을 유지하면 건강을 유지하고 비만이나 영양 실조와 같은 문제를 예방할 수 있습니다. 따라서 건강한 사람은 에너지 균형을 평형 상태로 유지하는 것이 중요하며, 이를 위해 1-2주의 기간 동안 식단과 운동을 조절해야 합니다.'}"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
