{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/ir/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine Similarity: 0.8726\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Hugging Face에서 colvert_v2 모델과 토크나이저 로드\n",
    "model_name = \"colbert-ir/colbertv2.0\"  # 실제 사용할 모델 이름으로 변경해야 합니다.\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "# 두 문장 설정\n",
    "sentence_1 = \"정육면체가 가라앉지 않고 물 위에 떠 있을 때 수면 윗부분에 해당하는 부피를 구하는 방법은?\"\n",
    "sentence_2 = \"물체의 부피가 2 × 10^-3m^3이고 무게가 6N인 물체가 물탱크에 띄워져 있습니다. 이 물체의 부피 중 얼마나 많은 부분이 물 위로 떠 있나요? 물체의 부피가 2 × 10^-3m^3이므로, 물체의 전체 부피는 100%입니다. 물 위로 떠 있는 부분의 부피를 구하기 위해선, 물체의 무게와 물의 밀도를 고려해야 합니다. 물체의 무게가 6N이므로, 물체의 무게는 물의 중력과 같습니다. 따라서, 물체의 부피 중 물 위로 떠 있는 부분의 무게는 물의 중력과 같습니다. 물의 밀도는 1g/cm^3이므로, 1cm^3의 물의 무게는 1g입니다. 따라서, 1m^3의 물의 무게는 1000kg입니다. 물체의 부피 중 물 위로 떠 있는 부분의 무게는 6N이므로, 이는 6N / 1000kg = 0.006kg입니다. 물체의 부피 중 물 위로 떠 있는 부분의 부피는 물체의 부피 중 물 위로 떠 있는 부분의 무게를 물의 밀도로 나눈 값입니다. 따라서, 물체의 부피 중 물 위로 떠 있는 부분의 부피는 0.006kg / 1000kg/m^3 = 0.006m^3입니다. 물체의 전체 부피가 2 × 10^-3m^3이므로, 물체의 부피 중 물 위로 떠 있는 부분의 비율은 (0.006m^3 / 2 × 10^-3m^3) × 100% = 300%입니다. 따라서, 물체의 부피 중 얼마나 많은 부분이 물 위로 떠 있나요? 70%입니다.\"\n",
    "\n",
    "# 문장을 토큰화 및 텐서로 변환\n",
    "inputs_1 = tokenizer(sentence_1, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "inputs_2 = tokenizer(sentence_2, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "\n",
    "# 모델에 입력하여 임베딩 추출\n",
    "with torch.no_grad():\n",
    "    embeddings_1 = model(**inputs_1).last_hidden_state.mean(dim=1)\n",
    "    embeddings_2 = model(**inputs_2).last_hidden_state.mean(dim=1)\n",
    "\n",
    "# 코사인 유사도 계산\n",
    "similarity = cosine_similarity(embeddings_1.numpy(), embeddings_2.numpy())\n",
    "\n",
    "# 유사도 출력\n",
    "print(f\"Cosine Similarity: {similarity[0][0]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine Similarity: 0.6764\n"
     ]
    }
   ],
   "source": [
    "sentence_1 = \"\"\"\n",
    "Everyone has the right to free speech.\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "sentence_2 = \"\"\"\n",
    "\n",
    "Be right and go right. \n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# 문장을 토큰화 및 텐서로 변환\n",
    "inputs_1 = tokenizer(sentence_1, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "inputs_2 = tokenizer(sentence_2, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "\n",
    "# 모델에 입력하여 임베딩 추출\n",
    "with torch.no_grad():\n",
    "    embeddings_1 = model(**inputs_1).last_hidden_state.mean(dim=1)\n",
    "    embeddings_2 = model(**inputs_2).last_hidden_state.mean(dim=1)\n",
    "\n",
    "# 코사인 유사도 계산\n",
    "similarity = cosine_similarity(embeddings_1.numpy(), embeddings_2.numpy())\n",
    "\n",
    "# 유사도 출력\n",
    "print(f\"Cosine Similarity: {similarity[0][0]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/ir/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.29856056]\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import CrossEncoder\n",
    "model = CrossEncoder('bongsoo/kpf-cross-encoder-v1')\n",
    "\n",
    "\n",
    "scores = model.predict([('오늘 날씨가 좋다', '오늘 등산을 한다')])\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7148747]\n"
     ]
    }
   ],
   "source": [
    "sentence_1 = \"\"\"\n",
    "나무의 분류에 대해 조사해 보기 위한 방법은?\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "sentence_2 = \"\"\"\n",
    "\n",
    "떡갈나무는 대체로 중간 크기의 나무로 알려져 있습니다. 그러나 학생들은 떡갈나무의 잎이 얼마나 큰지 궁금해합니다. 잎의 크기를 측정하는 가장 좋은 방법은 자 사용하기입니다. 자를 이용하여 잎의 길이와 너비를 정확하게 측정할 수 있습니다. 이렇게 얻은 데이터를 통해 학생들은 떡갈나무의 잎이 얼마나 큰지 알 수 있을 것입니다. 떡갈나무의 잎은 다양한 크기와 모양을 가지고 있으며, 자를 사용하여 측정하는 것은 정확한 결과를 얻을 수 있는 가장 효과적인 방법입니다. 따라서 학생들은 자를 사용하여 떡갈나무의 잎의 크기를 측정할 수 있습니다.\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "scores = model.predict([(sentence_1, sentence_2)])\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da0dffd461604938b0a3e80158e7bc27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.21k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70298e1e2a204025a15561f35cad676c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f764bcf3abe450d8cf5b9365053edb2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/17.1M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f652a06e9fab4314897810fdd306fb64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/963 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "632230ffd58345898744dc63ec8525a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/802 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8532f3d9518841b8ab82010341f9e52c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.24G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "XLMRobertaForSequenceClassification(\n",
       "  (roberta): XLMRobertaModel(\n",
       "    (embeddings): XLMRobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(250002, 1024, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 1024, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 1024)\n",
       "      (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): XLMRobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-23): 24 x XLMRobertaLayer(\n",
       "          (attention): XLMRobertaAttention(\n",
       "            (self): XLMRobertaSdpaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): XLMRobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): XLMRobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): XLMRobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (classifier): XLMRobertaClassificationHead(\n",
       "    (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (out_proj): Linear(in_features=1024, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_path = 'Dongjin-kr/ko-reranker'\n",
    "\n",
    "def exp_normalize(x):\n",
    "    b = x.max()\n",
    "    y = np.exp(x - b)\n",
    "    return y / y.sum()\n",
    "    \n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_path)\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_1 = \"\"\"\n",
    "나무의 분류에 대해 조사해 보기 위한 방법은?\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "sentence_2 = \"\"\"\n",
    "\n",
    "화학적 방어 메커니즘을 가진 동물들이 채택한 경계색은 Aposomatic 천연색입니다. Aposomatic 천연색은 동물들이 자신을 위협하는 포식자에게 경고 신호를 보내기 위해 사용하는 색상입니다. 이러한 색상은 독성 물질을 함유하고 있거나 독성을 가진 생물체일 가능성이 높은 동물들이 사용합니다. 이러한 경계색은 동물들이 포식자에게 위험하다는 메시지를 전달하고, 포식자들은 이러한 색상을 보고 위험을 느끼게 됩니다. 이를 통해 동물들은 포식자로부터의 공격을 피할 수 있습니다. Aposomatic 천연색은 동물들이 생존을 위해 발전한 효과적인 방어 메커니즘 중 하나입니다.\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first: -6.46032190322876, second: -3.325056314468384\n"
     ]
    }
   ],
   "source": [
    "# pairs = [[sentence_1, sentence_2]]\n",
    "pairs = [[\"나는 너를 싫어해\", \"나는 너를 사랑해\"], \\\n",
    "        [\"나는 너를 좋아해\", \"있어\"]]\n",
    "\n",
    "with torch.no_grad():\n",
    "        inputs = tokenizer(pairs, padding=True, truncation=True, return_tensors='pt', max_length=512)\n",
    "        scores = model(**inputs, return_dict=True).logits.view(-1, ).float()\n",
    "        scores = exp_normalize(scores.numpy())\n",
    "        print (f'first: {scores[0]}, second: {scores[1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first: -6.460322380065918, second: 0.7185262441635132\n"
     ]
    }
   ],
   "source": [
    "# pairs = [[sentence_1, sentence_2]]\n",
    "pairs = [[\"나는 너를 싫어해\", \"나는 너를 사랑해\"], \\\n",
    "        [\"나는 너를 좋아해\", \"너에 대한 나의 감정 일 수도 있어\"]]\n",
    "\n",
    "with torch.no_grad():\n",
    "        inputs = tokenizer(pairs, padding=True, truncation=True, return_tensors='pt', max_length=512)\n",
    "        scores = model(**inputs, return_dict=True).logits.view(-1, ).float()\n",
    "        scores = exp_normalize(scores.numpy())\n",
    "        print (f'first: {scores[0]}, second: {scores[1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_1 = \"\"\"\n",
    "나무의 분류에 대해 조사해 보기 위한 방법은?\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "sentence_2 = \"\"\"\n",
    "\n",
    "과학적으로 채소를 분류하는 데에는 다양한 기준이 있습니다. 채소는 종류에 따라 잎, 줄기, 뿌리 등으로 분류될 수 있습니다. 또한 채소의 색상, 크기, 모양 등도 분류 기준이 될 수 있습니다. 그러나 맛은 과학적으로 채소를 분류하는 데에는 사용되지 않는 기준입니다. 맛은 주관적인 경험에 의해 결정되기 때문에 과학적인 분류에는 적합하지 않습니다. 따라서 맛이 좋은지 여부는 채소를 분류하는 과학적인 기준으로 사용되지 않습니다.\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first: -6.180261135101318\n"
     ]
    }
   ],
   "source": [
    "pairs = [[sentence_1, sentence_2]]\n",
    "# pairs = [[\"나는 너를 싫어해\", \"나는 너를 사랑해\"], \\\n",
    "#         [\"나는 너를 좋아해\", \"너에 대한 나의 감정 일 수도 있어\"]]\n",
    "\n",
    "with torch.no_grad():\n",
    "        inputs = tokenizer(pairs, padding=True, truncation=True, return_tensors='pt', max_length=512)\n",
    "        scores = model(**inputs, return_dict=True).logits.view(-1, ).float()\n",
    "        # scores = exp_normalize(scores.numpy())\n",
    "        print (f'first: {scores[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ir",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
